ğŸ” How It Works (RAG Pipeline Summary)
This chatbot uses a Retrieval-Augmented Generation (RAG) workflow to answer medical questions accurately.

1ï¸âƒ£ Clean the textbook
PDF pages are extracted and cleaned
Headers, footers, noise, and page numbers removed
Saved as .txt files inside clean_pages/

2ï¸âƒ£ Chunk the text
Each page split into overlapping chunks (512â€“1024 tokens)
Helps retrieval for small and specific questions

3ï¸âƒ£ Create embeddings
Each chunk converted into a vector using OpenAI embeddings
Vectors capture meaning instead of keywords

4ï¸âƒ£ Store in FAISS index
All vectors stored in a FAISS vector database
Enables fast semantic similarity search

5ï¸âƒ£ Retrieve relevant chunks
When a user asks a question:
The question â†’ converted into an embedding
FAISS returns top-K relevant chunks
Page numbers + source file metadata attached

6ï¸âƒ£ LLM generates the answer
Retrieved chunks passed into the LLM
Model reads medical context
Produces grounded answer
Includes citations

7ï¸âƒ£ Streamlit UI
The interface displays:
The final answer
The supporting sources + page numbers
Button to view the full original page

ğŸ©º Example Queries (From SRB Surgery)
Try questions like:
â€œDefine acute inflammation.â€
â€œExplain the steps of coagulation.â€
â€œWhat are the symptoms of compartment syndrome?â€
â€œPage 74â€ (retrieve specific chunks)
